{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superclass of modules\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        h = f(z); z is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        \n",
    "        Returns:\n",
    "        output h\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        _gradOutput: dL/dh\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        # self._inputs saves the input of each layer\n",
    "        # self._inputs[i] is the input of i-th layer\n",
    "        self._inputs = [_input]\n",
    "        for i in range(self.size()):\n",
    "            self._inputs.append(self.layers[i].forward(self._inputs[i]))\n",
    "        self._output = self._inputs[-1]\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropogate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        # self._gradInputs[i] is the gradient of loss w.r.t. the input of i-th layer\n",
    "        self._gradInputs = [None] * (self.size() + 1)  \n",
    "        self._gradInputs[self.size()] = _gradOutput\n",
    "        for i in reversed(range(self.size())):\n",
    "            self._gradInputs[i] = self.layers[i].backward(self._inputs[i], self._gradInputs[i + 1])\n",
    "        self._gradInput = self._gradInputs[0]\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a nested list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            _p, _g = m.parameters()\n",
    "            if _p is not None:\n",
    "                params.append(_p)\n",
    "                gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        # Initalization\n",
    "        stdv = 1./np.sqrt(inputSize)\n",
    "        \n",
    "        self.weight = np.random.uniform(-stdv, stdv, (inputSize, outputSize))\n",
    "        self.gradWeight = np.zeros((inputSize, outputSize))\n",
    "        self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
    "        self.gradBias = np.zeros(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \n",
    "        _input:\n",
    "        N x inputSize matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        _input = _input.reshape(_input.shape[0], -1)\n",
    "\n",
    "        self._output = np.dot(_input, self.weight) + self.bias\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        _input:\n",
    "        N x inputSize matrix\n",
    "        _gradOutputSize:\n",
    "        N x outputSize matrix\n",
    "        \"\"\"\n",
    "        self.gradWeight = np.dot(_input.transpose(),_gradOutput)\n",
    "        self.gradBias = np.sum(_gradOutput,0)\n",
    "        self._gradInput = np.dot(_gradOutput,self.weight.transpose())\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \n",
    "        _input:\n",
    "        N x d matrix\n",
    "        \"\"\"\n",
    "        self._output = np.maximum(0, _input)\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \n",
    "        _input:\n",
    "        N x d matrix\n",
    "        \n",
    "        _gradOutput:\n",
    "        N x d matrix\n",
    "        \"\"\"\n",
    "        mask = _input > 0\n",
    "        self._gradInput = _gradOutput * mask\n",
    "        return self._gradInput\n",
    "\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Logistic(Module):\n",
    "    \"\"\"\n",
    "    Logistic activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self._input = _input\n",
    "        self._output = 1. /  (1 + np.exp(-self._input))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = _gradOutput * (1. - self._output) * self._output\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        # YOUR CODE HERE\n",
    "        # Need to take care of training mode and evaluation mode\n",
    "        if self.p > 0:\n",
    "            if self.train:\n",
    "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
    "                self.mask /= 1 - self.p\n",
    "                self._output *= self.mask\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        if self.train:\n",
    "            if self.p > 0:\n",
    "                self._gradInput *= self.mask\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what in hw2)\n",
    "\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        self._input = _input - np.max(_input, axis=1, keepdims=True)\n",
    "        self._logprob = self._input - np.log(np.exp(self._input).sum(axis=1, keepdims=True))\n",
    "        self._output = np.mean(np.sum(-self._logprob * _label, axis=1))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _label):\n",
    "        self._gradInput = (np.exp(self._logprob) - _label) / _label.shape[0]\n",
    "        return self._gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03419546 -0.30100657  0.02007918  0.04038765  0.02561507  0.03003862\n",
      "   0.02748343  0.04419535  0.03635619  0.04265563]\n",
      " [ 0.04053102  0.04722263 -0.31209795  0.05100902  0.02184379  0.03082003\n",
      "   0.02457761  0.0324217   0.02202906  0.04164309]\n",
      " [ 0.03689008  0.02017514  0.04039379 -0.30474866  0.03439409  0.05277443\n",
      "   0.02983538  0.03036936  0.03903806  0.02087834]]\n",
      "[[ 0.03419546 -0.30100657  0.02007918  0.04038765  0.02561507  0.03003862\n",
      "   0.02748343  0.04419535  0.03635619  0.04265563]\n",
      " [ 0.04053102  0.04722263 -0.31209795  0.05100902  0.02184379  0.03082003\n",
      "   0.02457761  0.0324217   0.02202906  0.04164309]\n",
      " [ 0.03689008  0.02017514  0.04039379 -0.30474866  0.03439409  0.05277443\n",
      "   0.02983538  0.03036936  0.03903806  0.02087834]]\n",
      "5.6386797378842845e-09\n"
     ]
    }
   ],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    crit.forward(x, gt)\n",
    "\n",
    "    gradInput = crit.backward(x, gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(gradInput)\n",
    "    print(gradInput_num)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing FullyConnected\n",
      "1.1215437853340808e-08\n",
      "testing ReLU\n",
      "6.988901124005461e-11\n",
      "testing dropout\n",
      "1.0\n",
      "1.1236146222602319e-08\n"
     ]
    }
   ],
   "source": [
    "# Test modules, all the relative errors should be small enough\n",
    "def test_module(model):\n",
    "\n",
    "    #model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    gradInput = model.backward(x, crit.backward(model.forward(x), gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "print('testing FullyConnected')\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "print('testing ReLU')\n",
    "model = ReLU()\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "print(\"testing dropout\")\n",
    "model = Dropout()\n",
    "test_module(model)\n",
    "# You can only test dropout in evaluation mode.\n",
    "\n",
    "# Test Sequential\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(10, 10))\n",
    "model.add(ReLU())\n",
    "model.add(FullyConnected(10, 10))\n",
    "#model.add(Dropout())\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3943679846996052\n",
      "0.013860989469357432\n",
      "0.01092164763738045\n",
      "0.013228630714962119\n",
      "0.012597156017384762\n",
      "0.012148466808700612\n",
      "0.01204733344916826\n",
      "0.011837429413871229\n",
      "0.013465227329659346\n",
      "0.010133790280275251\n",
      "0.014605215658158779\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "#model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    params, gradParams = model.parameters()\n",
    "    sgdmom(params, gradParams, 0.01, 0.8)\n",
    "    if it > 1000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(7000, 576)\n",
      "(7000, 10)\n",
      "Load valset.\n",
      "(2000, 576)\n",
      "(2000, 10)\n"
     ]
    }
   ],
   "source": [
    "import MNIST_utils\n",
    "data_fn = \"CLEAN_MNIST_SUBSETS.h5\"\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = MNIST_utils.load_data(data_fn, \"large_train\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = MNIST_utils.load_data(data_fn, \"val\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    \"\"\"\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        bX = X[shuffled_idx[batch_idx],:]\n",
    "        bY = Y[shuffled_idx[batch_idx],:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # print(loss)\n",
    "        dscore = crit.backward(score, bY)\n",
    "        model.backward(bX, dscore)\n",
    "        \n",
    "        # Update the data using \n",
    "        params, gradParams = model.parameters()\n",
    "        sgdmom(params, gradParams, eta, weight_decay = trainopt['lambda'])    \n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build the model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector\n",
    "    output_size: the output size of final layer.\n",
    "    activation_func: ReLU, Logistic, Tanh, etc. (Need to be implemented by yourself)\n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(FullyConnected(input_size, hidden_size))\n",
    "\n",
    "    if activation_func == 'ReLU':\n",
    "        model.add(ReLU())\n",
    "    elif activation_func == 'Logistic':\n",
    "        model.add(Logistic())\n",
    "\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(FullyConnected(hidden_size, output_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.308 train error: 90.529 val error: 90.600\n",
      "     500 batch loss: 0.440 train error: 10.671 val error: 10.000\n",
      "    1000 batch loss: 0.317 train error: 8.571 val error: 8.300\n",
      "    1500 batch loss: 0.250 train error: 7.457 val error: 7.450\n",
      "    2000 batch loss: 0.239 train error: 6.629 val error: 7.300\n",
      "    2500 batch loss: 0.156 train error: 6.186 val error: 7.200\n",
      "    3000 batch loss: 0.305 train error: 5.857 val error: 7.250\n",
      "    3500 batch loss: 0.170 train error: 5.486 val error: 7.100\n",
      "    4000 batch loss: 0.160 train error: 5.186 val error: 7.050\n",
      "    4500 batch loss: 0.161 train error: 4.657 val error: 6.900\n",
      "    5000 batch loss: 0.143 train error: 4.371 val error: 7.150\n",
      "    5500 batch loss: 0.176 train error: 4.243 val error: 7.150\n",
      "    6000 batch loss: 0.080 train error: 4.014 val error: 6.950\n",
      "    6500 batch loss: 0.242 train error: 3.943 val error: 7.150\n",
      "    7000 batch loss: 0.100 train error: 3.857 val error: 7.400\n",
      "    7500 batch loss: 0.108 train error: 3.757 val error: 7.300\n",
      "    8000 batch loss: 0.118 train error: 3.457 val error: 7.350\n",
      "    8500 batch loss: 0.116 train error: 3.471 val error: 7.450\n",
      "    9000 batch loss: 0.160 train error: 3.271 val error: 7.500\n",
      "    9500 batch loss: 0.061 train error: 3.114 val error: 7.400\n",
      "   10000 batch loss: 0.215 train error: 3.043 val error: 7.350\n",
      "   10500 batch loss: 0.076 train error: 2.971 val error: 7.450\n",
      "   11000 batch loss: 0.087 train error: 2.971 val error: 7.450\n",
      "   11500 batch loss: 0.096 train error: 2.857 val error: 7.550\n",
      "   12000 batch loss: 0.102 train error: 2.886 val error: 7.850\n",
      "   12500 batch loss: 0.147 train error: 2.786 val error: 7.600\n",
      "   13000 batch loss: 0.053 train error: 2.686 val error: 7.350\n",
      "   13500 batch loss: 0.200 train error: 2.571 val error: 7.400\n",
      "   14000 batch loss: 0.067 train error: 2.529 val error: 7.650\n",
      "   14500 batch loss: 0.076 train error: 2.514 val error: 7.600\n",
      "   15000 batch loss: 0.085 train error: 2.529 val error: 7.450\n",
      "   15500 batch loss: 0.095 train error: 2.471 val error: 7.500\n",
      "   16000 batch loss: 0.139 train error: 2.471 val error: 7.450\n",
      "   16500 batch loss: 0.048 train error: 2.457 val error: 7.350\n",
      "   17000 batch loss: 0.189 train error: 2.386 val error: 7.300\n",
      "   17500 batch loss: 0.062 train error: 2.314 val error: 7.600\n",
      "   18000 batch loss: 0.069 train error: 2.300 val error: 7.550\n",
      "   18500 batch loss: 0.080 train error: 2.300 val error: 7.450\n",
      "   19000 batch loss: 0.090 train error: 2.286 val error: 7.500\n",
      "   19500 batch loss: 0.136 train error: 2.300 val error: 7.400\n",
      "train set model: -> lambda= 0.0100, train error: 2.30, val error: 6.90\n",
      "       0 batch loss: 2.334 train error: 89.771 val error: 90.700\n",
      "     500 batch loss: 0.327 train error: 10.857 val error: 10.150\n",
      "    1000 batch loss: 0.327 train error: 8.543 val error: 8.300\n",
      "    1500 batch loss: 0.229 train error: 7.357 val error: 7.550\n",
      "    2000 batch loss: 0.277 train error: 6.971 val error: 7.700\n",
      "    2500 batch loss: 0.256 train error: 6.400 val error: 6.850\n",
      "    3000 batch loss: 0.152 train error: 6.000 val error: 7.450\n",
      "    3500 batch loss: 0.197 train error: 5.671 val error: 7.300\n",
      "    4000 batch loss: 0.093 train error: 5.343 val error: 7.200\n",
      "    4500 batch loss: 0.179 train error: 5.143 val error: 7.100\n",
      "    5000 batch loss: 0.148 train error: 4.586 val error: 7.450\n",
      "    5500 batch loss: 0.179 train error: 4.686 val error: 7.450\n",
      "    6000 batch loss: 0.155 train error: 4.500 val error: 7.250\n",
      "    6500 batch loss: 0.108 train error: 4.500 val error: 7.700\n",
      "    7000 batch loss: 0.144 train error: 4.243 val error: 7.450\n",
      "    7500 batch loss: 0.070 train error: 4.143 val error: 7.500\n",
      "    8000 batch loss: 0.153 train error: 4.014 val error: 7.300\n",
      "    8500 batch loss: 0.122 train error: 3.943 val error: 7.500\n",
      "    9000 batch loss: 0.144 train error: 4.000 val error: 7.600\n",
      "    9500 batch loss: 0.121 train error: 3.886 val error: 7.600\n",
      "   10000 batch loss: 0.092 train error: 3.743 val error: 7.550\n",
      "   10500 batch loss: 0.127 train error: 3.657 val error: 7.600\n",
      "   11000 batch loss: 0.059 train error: 3.629 val error: 7.700\n",
      "   11500 batch loss: 0.144 train error: 3.443 val error: 7.500\n",
      "   12000 batch loss: 0.107 train error: 3.386 val error: 7.650\n",
      "   12500 batch loss: 0.118 train error: 3.500 val error: 7.600\n",
      "   13000 batch loss: 0.103 train error: 3.500 val error: 7.600\n",
      "   13500 batch loss: 0.086 train error: 3.343 val error: 7.650\n",
      "   14000 batch loss: 0.118 train error: 3.143 val error: 7.650\n",
      "   14500 batch loss: 0.053 train error: 3.171 val error: 7.700\n",
      "   15000 batch loss: 0.137 train error: 3.086 val error: 7.600\n",
      "   15500 batch loss: 0.098 train error: 3.100 val error: 7.600\n",
      "   16000 batch loss: 0.105 train error: 3.114 val error: 7.600\n",
      "   16500 batch loss: 0.093 train error: 3.000 val error: 7.600\n",
      "   17000 batch loss: 0.082 train error: 3.114 val error: 7.650\n",
      "   17500 batch loss: 0.113 train error: 2.957 val error: 7.600\n",
      "   18000 batch loss: 0.050 train error: 2.971 val error: 7.600\n",
      "   18500 batch loss: 0.133 train error: 2.943 val error: 7.650\n",
      "   19000 batch loss: 0.095 train error: 2.900 val error: 7.700\n",
      "   19500 batch loss: 0.099 train error: 2.857 val error: 7.600\n",
      "train set model: -> lambda= 0.0250, train error: 2.86, val error: 6.85\n",
      "       0 batch loss: 2.300 train error: 92.000 val error: 92.200\n",
      "     500 batch loss: 0.452 train error: 11.686 val error: 10.650\n",
      "    1000 batch loss: 0.286 train error: 9.100 val error: 8.450\n",
      "    1500 batch loss: 0.417 train error: 7.800 val error: 8.300\n",
      "    2000 batch loss: 0.314 train error: 7.314 val error: 7.900\n",
      "    2500 batch loss: 0.189 train error: 6.814 val error: 7.800\n",
      "    3000 batch loss: 0.313 train error: 6.500 val error: 7.850\n",
      "    3500 batch loss: 0.137 train error: 5.786 val error: 7.750\n",
      "    4000 batch loss: 0.188 train error: 5.443 val error: 8.100\n",
      "    4500 batch loss: 0.132 train error: 5.314 val error: 8.400\n",
      "    5000 batch loss: 0.299 train error: 4.800 val error: 8.450\n",
      "    5500 batch loss: 0.218 train error: 4.757 val error: 8.100\n",
      "    6000 batch loss: 0.148 train error: 4.500 val error: 8.400\n",
      "    6500 batch loss: 0.236 train error: 4.386 val error: 8.400\n",
      "    7000 batch loss: 0.085 train error: 4.186 val error: 8.050\n",
      "    7500 batch loss: 0.165 train error: 4.100 val error: 7.650\n",
      "    8000 batch loss: 0.097 train error: 3.943 val error: 8.050\n",
      "    8500 batch loss: 0.237 train error: 3.814 val error: 8.050\n",
      "    9000 batch loss: 0.173 train error: 3.643 val error: 7.900\n",
      "    9500 batch loss: 0.129 train error: 3.629 val error: 8.000\n",
      "   10000 batch loss: 0.197 train error: 3.600 val error: 8.100\n",
      "   10500 batch loss: 0.067 train error: 3.329 val error: 7.800\n",
      "   11000 batch loss: 0.155 train error: 3.314 val error: 7.600\n",
      "   11500 batch loss: 0.079 train error: 3.329 val error: 7.700\n",
      "   12000 batch loss: 0.202 train error: 3.229 val error: 7.950\n",
      "   12500 batch loss: 0.145 train error: 3.100 val error: 7.950\n",
      "   13000 batch loss: 0.115 train error: 3.086 val error: 7.950\n",
      "   13500 batch loss: 0.176 train error: 2.971 val error: 8.000\n",
      "   14000 batch loss: 0.060 train error: 3.057 val error: 7.850\n",
      "   14500 batch loss: 0.149 train error: 3.071 val error: 7.800\n",
      "   15000 batch loss: 0.070 train error: 2.914 val error: 7.900\n",
      "   15500 batch loss: 0.181 train error: 2.829 val error: 7.950\n",
      "   16000 batch loss: 0.129 train error: 2.829 val error: 8.050\n",
      "   16500 batch loss: 0.108 train error: 2.843 val error: 8.050\n",
      "   17000 batch loss: 0.162 train error: 2.729 val error: 8.150\n",
      "   17500 batch loss: 0.056 train error: 2.729 val error: 8.100\n",
      "   18000 batch loss: 0.145 train error: 2.729 val error: 8.000\n",
      "   18500 batch loss: 0.066 train error: 2.700 val error: 8.000\n",
      "   19000 batch loss: 0.172 train error: 2.686 val error: 7.950\n",
      "   19500 batch loss: 0.122 train error: 2.686 val error: 8.000\n",
      "train set model: -> lambda= 0.0500, train error: 2.69, val error: 7.60\n",
      "       0 batch loss: 2.291 train error: 86.986 val error: 87.150\n",
      "     500 batch loss: 0.424 train error: 11.014 val error: 9.900\n",
      "    1000 batch loss: 0.389 train error: 8.900 val error: 8.450\n",
      "    1500 batch loss: 0.308 train error: 7.829 val error: 7.600\n",
      "    2000 batch loss: 0.230 train error: 6.943 val error: 7.200\n",
      "    2500 batch loss: 0.223 train error: 6.257 val error: 7.000\n",
      "    3000 batch loss: 0.152 train error: 5.800 val error: 7.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3500 batch loss: 0.125 train error: 5.400 val error: 7.150\n",
      "    4000 batch loss: 0.124 train error: 5.043 val error: 7.200\n",
      "    4500 batch loss: 0.227 train error: 4.986 val error: 7.050\n",
      "    5000 batch loss: 0.224 train error: 4.586 val error: 7.200\n",
      "    5500 batch loss: 0.175 train error: 4.500 val error: 6.750\n",
      "    6000 batch loss: 0.137 train error: 4.186 val error: 7.000\n",
      "    6500 batch loss: 0.109 train error: 4.043 val error: 6.950\n",
      "    7000 batch loss: 0.087 train error: 3.886 val error: 6.900\n",
      "    7500 batch loss: 0.089 train error: 3.743 val error: 7.000\n",
      "    8000 batch loss: 0.171 train error: 3.671 val error: 6.750\n",
      "    8500 batch loss: 0.187 train error: 3.514 val error: 6.900\n",
      "    9000 batch loss: 0.145 train error: 3.557 val error: 6.800\n",
      "    9500 batch loss: 0.117 train error: 3.429 val error: 6.850\n",
      "   10000 batch loss: 0.094 train error: 3.214 val error: 6.950\n",
      "   10500 batch loss: 0.072 train error: 3.071 val error: 6.900\n",
      "   11000 batch loss: 0.073 train error: 3.100 val error: 7.050\n",
      "   11500 batch loss: 0.144 train error: 3.057 val error: 7.050\n",
      "   12000 batch loss: 0.160 train error: 3.043 val error: 7.050\n",
      "   12500 batch loss: 0.121 train error: 2.900 val error: 7.000\n",
      "   13000 batch loss: 0.107 train error: 2.943 val error: 7.150\n",
      "   13500 batch loss: 0.087 train error: 2.800 val error: 7.300\n",
      "   14000 batch loss: 0.067 train error: 2.843 val error: 7.100\n",
      "   14500 batch loss: 0.066 train error: 2.729 val error: 7.050\n",
      "   15000 batch loss: 0.129 train error: 2.729 val error: 7.150\n",
      "   15500 batch loss: 0.142 train error: 2.657 val error: 7.000\n",
      "   16000 batch loss: 0.106 train error: 2.600 val error: 7.050\n",
      "   16500 batch loss: 0.101 train error: 2.557 val error: 7.000\n",
      "   17000 batch loss: 0.082 train error: 2.500 val error: 7.150\n",
      "   17500 batch loss: 0.062 train error: 2.586 val error: 7.050\n",
      "   18000 batch loss: 0.061 train error: 2.557 val error: 7.050\n",
      "   18500 batch loss: 0.122 train error: 2.457 val error: 7.100\n",
      "   19000 batch loss: 0.136 train error: 2.486 val error: 7.200\n",
      "   19500 batch loss: 0.101 train error: 2.471 val error: 7.150\n",
      "train set model: -> lambda= 0.0750, train error: 2.47, val error: 6.75\n",
      "       0 batch loss: 2.329 train error: 85.600 val error: 85.900\n",
      "     500 batch loss: 0.467 train error: 10.571 val error: 9.500\n",
      "    1000 batch loss: 0.334 train error: 8.357 val error: 8.350\n",
      "    1500 batch loss: 0.144 train error: 7.257 val error: 8.000\n",
      "    2000 batch loss: 0.155 train error: 6.486 val error: 7.350\n",
      "    2500 batch loss: 0.167 train error: 5.957 val error: 6.950\n",
      "    3000 batch loss: 0.123 train error: 5.843 val error: 7.150\n",
      "    3500 batch loss: 0.215 train error: 5.386 val error: 7.150\n",
      "    4000 batch loss: 0.300 train error: 5.229 val error: 7.300\n",
      "    4500 batch loss: 0.161 train error: 4.871 val error: 7.250\n",
      "    5000 batch loss: 0.077 train error: 4.714 val error: 7.350\n",
      "    5500 batch loss: 0.087 train error: 4.329 val error: 6.900\n",
      "    6000 batch loss: 0.128 train error: 4.214 val error: 6.900\n",
      "    6500 batch loss: 0.084 train error: 4.229 val error: 7.350\n",
      "    7000 batch loss: 0.160 train error: 4.043 val error: 7.150\n",
      "    7500 batch loss: 0.207 train error: 3.900 val error: 7.350\n",
      "    8000 batch loss: 0.126 train error: 3.771 val error: 7.300\n",
      "    8500 batch loss: 0.060 train error: 3.786 val error: 7.150\n",
      "    9000 batch loss: 0.071 train error: 3.471 val error: 6.950\n",
      "    9500 batch loss: 0.122 train error: 3.486 val error: 7.300\n",
      "   10000 batch loss: 0.074 train error: 3.457 val error: 7.400\n",
      "   10500 batch loss: 0.134 train error: 3.229 val error: 7.350\n",
      "   11000 batch loss: 0.156 train error: 3.157 val error: 7.100\n",
      "   11500 batch loss: 0.107 train error: 3.186 val error: 7.200\n",
      "   12000 batch loss: 0.052 train error: 3.114 val error: 7.150\n",
      "   12500 batch loss: 0.064 train error: 3.029 val error: 7.250\n",
      "   13000 batch loss: 0.120 train error: 2.929 val error: 7.350\n",
      "   13500 batch loss: 0.066 train error: 2.900 val error: 7.350\n",
      "   14000 batch loss: 0.122 train error: 2.786 val error: 7.400\n",
      "   14500 batch loss: 0.134 train error: 2.800 val error: 7.100\n",
      "   15000 batch loss: 0.096 train error: 2.671 val error: 7.150\n",
      "   15500 batch loss: 0.048 train error: 2.600 val error: 7.250\n",
      "   16000 batch loss: 0.060 train error: 2.643 val error: 7.400\n",
      "   16500 batch loss: 0.119 train error: 2.543 val error: 7.300\n",
      "   17000 batch loss: 0.062 train error: 2.686 val error: 7.450\n",
      "   17500 batch loss: 0.114 train error: 2.614 val error: 7.500\n",
      "   18000 batch loss: 0.116 train error: 2.543 val error: 7.250\n",
      "   18500 batch loss: 0.092 train error: 2.471 val error: 7.300\n",
      "   19000 batch loss: 0.047 train error: 2.443 val error: 7.250\n",
      "   19500 batch loss: 0.058 train error: 2.429 val error: 7.400\n",
      "train set model: -> lambda= 0.1000, train error: 2.43, val error: 6.90\n",
      "lambda= 0.0100, hidden size:    10, val error: 6.90\n",
      "lambda= 0.0250, hidden size:    10, val error: 6.85\n",
      "lambda= 0.0500, hidden size:    10, val error: 7.60\n",
      "lambda= 0.0750, hidden size:    10, val error: 6.75\n",
      "lambda= 0.1000, hidden size:    10, val error: 6.90\n",
      "Best train model val err: 6.75\n",
      "Best train model lambda: 0.075\n"
     ]
    }
   ],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': .1,   # initial learning rate\n",
    "    'maxiter': 20000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 500,  # display batch loss every display_iter updates\n",
    "    'batch_size': 100,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25  #\n",
    "}\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# set the (initial?) set of lambda values to explore\n",
    "lambdas = np.array([0.01, 0.025, 0.05, 0.075, 0.1])\n",
    "hidden_sizes = np.array([10])\n",
    "    \n",
    "for lambda_ in lambdas:\n",
    "    for hidden_size_ in hidden_sizes:\n",
    "        trainopt['lambda'] = lambda_\n",
    "        model = build_model(NFEATURES, hidden_size_, 10, dropout = 0)\n",
    "        crit = SoftMaxLoss()\n",
    "        # -- model trained on large train set\n",
    "        trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "        trained_models[(lambda_, hidden_size_)] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "        print('train set model: -> lambda= %.4f, train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    \n",
    "best_trained_lambda = 0.\n",
    "best_trained_model = None\n",
    "best_trained_val_err = 100.\n",
    "for (lambda_, hidden_size_), results in trained_models.items():\n",
    "    print('lambda= %.4f, hidden size: %5d, val error: %.2f' %(lambda_, hidden_size_, results['val_err']))\n",
    "    if results['val_err'] < best_trained_val_err:\n",
    "        best_trained_val_err = results['val_err']\n",
    "        best_trained_model = results['model']\n",
    "        best_trained_lambda = lambda_\n",
    "\n",
    "print(\"Best train model val err:\", best_trained_val_err)\n",
    "print(\"Best train model lambda:\", best_trained_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Kaggle submission file using `model`\n",
    "kaggleX = MNIST_utils.load_data(data_fn, 'kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_trained_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SGD, the performance was:\n",
    "\n",
    "- best train set model: -> lambda= 0.1000, train error: 2.30, val error: 6.85\n",
    "- lambda= 0.0000, hidden size: 10, val error: 7.10\n",
    "- lambda= 0.0010, hidden size: 10, val error: 7.05\n",
    "- lambda= 0.0100, hidden size: 10, val error: 7.30\n",
    "- lambda= 0.1000, hidden size: 10, val error: 6.85\n",
    "\n",
    "For SGD with momentum:\n",
    "\n",
    "- best train set model: -> lambda= 0.1000, train error: 2.51, val error: 6.70\n",
    "- lambda= 0.0000, hidden size: 10, val error: 7.30\n",
    "- lambda= 0.0010, hidden size: 10, val error: 6.80\n",
    "- lambda= 0.0100, hidden size: 10, val error: 7.60\n",
    "- lambda= 0.1000, hidden size: 10, val error: 6.70\n",
    "\n",
    "For SGD with Nesterov momentum:\n",
    "\n",
    "- train set model: -> lambda= 0.0200, train error: 2.49, val error: 6.60\n",
    "- lambda= 0.0000, hidden size:    10, val error: 7.10\n",
    "- lambda= 0.0500, hidden size:    10, val error: 7.15\n",
    "- lambda= 0.0100, hidden size:    10, val error: 7.30\n",
    "- lambda= 0.0150, hidden size:    10, val error: 7.40\n",
    "- lambda= 0.0200, hidden size:    10, val error: 6.60\n",
    "\n",
    "Overall, SGD with Nesterov momentum outperforms the other two algorithms in terms of achieving the lowest validation error. This suggests that incorporating Nesterov momentum into the optimization process helps in finding a better solution for the linear regression problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
